<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>来吧！</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="/css/docs.css" rel="stylesheet" type="text/css">
    <link href="/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/css/theme.css" rel="stylesheet" type="text/css">
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/docs.min.js"></script>
</head>

<body>
<header id="top">
    <div class="row-fluid">
        <div class="navbar navbar-inverse" role="navigation">
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li class="active"><a href="/">Home</a></li>
                    <li class="active"><a href="/article">笔记</a></li>
                    <li class="active"><a href="/about">About</a></li>
                </ul>
            </div>
        </div>
    </div>
</header>


<div class="container-fluid">
    <div class="row">
        <div class="col-md-2 hidden-xs">
            <div class="sidebar well">
    
</div>

        </div>
        <div class="col-md-8">
            <div class="article">
                <div class="well">
                    <h1 class="none title">数据及服务迁移记录</h1>
                    <div class="text-muted time">
                        <span>发布时间：2017年09月08日</span>
                        <span class="col-md-offset-1">作者：龙飞</span>
                    </div>
                    <div class="content">
                        <p>来到头条半年多，从游戏行业跨入互联网行业，看起来差别不大，个中滋味非经历不能体会。除了都用Python写一些东西之外，其他种种都要学习。沉默术士说得好，watch and learn.</p>

<p>这半年参与的最重要的事情就是数据库和数据服务的迁移。当初接到这个任务的时候，天真地以为至多两个月就能搞定，最终却几乎耗费了三倍的时间。这过程中包含了学习的开销，需求的变更带来的花费，与上游沟通的成本，当然更少不了走弯路带来的不必要的额外花销。趁着还没有忘记主要流程，把这部分内容写下来，是有此文。</p>
<h1 id="我们要做什么">我们要做什么</h1>
<p>重构之前，我们的数据服务长下图左边这样。
<img src="../../../video_db.png" alt="" />
old_db库存放所有视频相关数据，比如原视频存放地址，转码后视频地址，视频数据元信息等。old_service为原来读写数据的服务。vod是点播读取视频地址的服务，只有读请求。others表示其他通过old_service服务访问DB的请求。wildlings表示切换到微服务之前，原来直接读写数据服务的Python客户端。</p>

<p>上述结构的问题是：</p>

<p>• 数据访问不收敛，一个需求变更可能需要多处改动。因为有些访问缺乏统计和监控，出问题时也不方便迅速定位。</p>

<p>• 原数据库表结构设计不十分合理，比如转码后视频和原视频分开存储，前者以后者在数据表中的自增ID作为索引。但实际使用中都是使用video_id(32位的uuid)，如果要用一个video_id查出视频相关数据，则需要两次查询。</p>

<p>• 还有个相关的原因是，之前转码使用的celery做任务调度，但是当视频量逐渐上升之后，渐渐不能满足需求了。</p>

<p>为此，我们决定把数据库重新设计。同时将所有访问都收敛到一个新的数据层服务。重构之后长右边的样子。这样就清爽多了。</p>
<h1 id="怎么做">怎么做</h1>

<ol>
  <li>
    <p>DB重构</p>

    <p>此处不表</p>
  </li>
  <li>
    <p>数据服务重构上线</p>

    <p>前面已经提到过了，原服务叫做old_service，重构后的服务叫new_service。</p>

    <p>data_accss服务使用了两层Redis做cache，一层为缓存型，过期时间较短，一层为持久型，过期时间较长。设计的目的是尽量减少打到DB的流量。一个典型的读数据流程如上图所示。这种设计要求第二层缓存容量显著大于第一层容量才能有效利用空间，然而我们的线上环境并没有配置成这样，所以第二层Redis的命中率长期保持在20%以下。</p>

    <p>new_service基本上继承了old_service的结构，区别有三处。</p>

    <p>• 去掉第二层Redis，改用Abase。后者是在线KV数据库，容量足够大，足以容纳一年以上的视频数据。</p>

    <p>• 增加一个MD5Redis，用来存储上传文件的MD5，用作消重。</p>

    <p>• 每当DB有写操作时，都会写入NSQ。上层vod会消费NSQ，写入自己的缓存，用于容灾。</p>

    <p>new_service还需要考虑另外一个问题，如何不着痕迹地替换线上一直运行的old_service？有两个方案可选。</p>

    <p>• 快速上线，直接切换服务，在某个指定时间段内完成。但考虑到我们15k+的qps，以及分散的上游调用，这种方法很难实现。</p>

    <p>• 用较长的一段时间来进行切换，双写双读两个数据库，校验数据准确率达到4个9以上之后进行切换。</p>

    <p>我们很自然地采用了第二种方案，为了不影响视频的正常播放，我们决定先切写，稳定后再切读流量。好怀念做游戏的时候可以停服维护的日子！</p>

    <div class="highlighter-rouge"><pre class="highlight"><code>######################################################################################################
漫长的线下测试对比过程，保证数据100%match（实际不是100%，因为新库会删掉一些不需要的字段，另加一些字段，这些在对比时忽略）......
######################################################################################################
</code></pre>
    </div>

    <p>是时候把我们的服务部署上线了，注意这一步还没有接入流量。</p>

    <p>这种方案也有需要思考的地方：双读不用说，肯定是先读新库，如果没有再读老库。双写是先写新库还是先写老库呢？我们决定先写新库，因为最终的目的是使用新库，如果写入有错能尽早地暴露问题。The sooner, the better.</p>

    <p>为了保证写入新库数据的正确性，我们需要足够长的时间来进行数据校验。</p>

    <p>为了保证线上数据正确性，必须两个库都写成功才回给客户端返回成功。毫无疑问，这增加了我们服务出错的概率，因此我们需要尽量缩短双写状态持续的时间。可见上述两个要求是相互矛盾的。</p>
  </li>
  <li>
    <p>收敛部分线上流量到new_service</p>

    <p>warning: 前面两步都是离线操作，随时可以删除重来。但是从这一步开始，以后的操作都跟线上环境相关，必须小心谨慎。</p>

    <p>这里的部分流量指的是指那些还没有收敛到old_service服务的写流量。这只占所有流量很小的一部分，即使有影响也比较可控。</p>

    <p>同样也是因为流量较小，我们的对比工作进行得不太顺利。所以我们需要更多的流量。</p>
  </li>
  <li>
    <p>切换转码流量到new_service</p>

    <p>前面提到了，除了用户上传的原始视频之外，转码后的视频数据也是DB写入量的一大来源(事实上因为一个原始视频会转出多个分辨率，转码后视频数据量是原视频的若干倍）。</p>

    <p>转码的全部输入来自于用户上传的数据，只要上传数据不丢，即使转码失败也可以修复。如果上传写入失败，那么这个视频就无法正确放出了。所以我们选择先迁移转码的写流量。</p>

    <p>如何区分上传和转码的流量呢，我们将new_service部署了default和write两个集群，前者用于上传，后者用于转码，在代码中控制。</p>
  </li>
  <li>
    <p>切换上传流量到new_service</p>

    <p>在测试对比将近两周之后，我们认为是时候完全收敛写流量了。请注意，这里要面对一个很严峻的问题：如果切换上传失败，那么用户在这一段时间的数据将会永远消失，没有任何恢复的办法。</p>

    <p>为此，我们作了“周密”的计划： 在23:00以后切换，因为这段时间上传量达到每天的最小值。</p>

    <p>事实证明，这是一个英明的决策，我们大概花了一个通宵的时间来完成这个过程。期间各种trail and error，大致原因是上传也有若干版本，没有完全收敛。具体细节记不太清楚了，这也说明了事后马上记录的意义。</p>
  </li>
  <li>
    <p>数据迁移，读接口校验</p>

    <p>这里我们又到了一个比较轻松的阶段，只需要定期从新库和老库中取出数据对比即可。除此之外，还有一个看起来简单却异常麻烦的工作：导数据。因为服务现在采用双写，所以只需要导入切流量之前的数据即可。</p>

    <p>我们的数据库存储了2015年头条的第一条视频到现在的所有数据，并且每天以百万级别的量（写文章的时候已超过500万）在增加。总共约3亿条（这里单指原始视频，如果加上转码后视频，会再增加一个数量级）。</p>

    <p>同时虽然old_service服务加了两次cache，但是还是有上千的QPS打到后端DB。如何能快速导完数据并且不影响线上流量？多进程+读从库可破之。之所以使用多进程是因为人生苦短，我用Python。</p>

    <p>有一段时间开发机上将近40个100%CPU的Python进程就是我的作品。</p>

    <p>然而导完了只不过完成了一小部分工作，主要工作是不停地对比。比到山无棱，天地合。对比并不是无意义的，每次都能扫出一些不一样的东西。</p>

    <p>这一步的主要问题有：</p>

    <p>• 导数据中断如何恢复？因为每个进程的数据是预先划分好的，所以每个进程用一个文件记录导入进度即可。</p>

    <p>• 脏数据清洗。因为老DB没有加Uniq索引，所以会出现重复数据，同时还会有一些残缺数据，以及无用数据（比如未上传的）。这一步我们去掉了很多很久以前转码未成功的视频以及缺乏各种必要参数（如存储地址）的视频。对于未转码的视频，我们采用了导入后重转码的方案。</p>

    <p>• 如何找到合适的进程数和单次读取DB条数? Trial and error，时刻关注监控。</p>
  </li>
  <li>
    <p>切换到Sharding数据库,导数据again</p>

    <p>在数据还没对比完的时候，我们发现一个问题：按照目前这种增长速度，单个DB很快就不够用了。于是引入了分库，这个库被命名为sharding库。</p>

    <p>所以我们的写流量需要切换到sharding库，然后下掉new_db库。</p>

    <p>自然，切完流量后，导数据流向就变成了从old_db库到Sharding库。</p>

    <p>sharding库是用Mycat作为代理的。关于Mycat，要吐槽的地方实在太多了，此处省略。</p>
  </li>
  <li>
    <p>上游读对比</p>

    <p>OK，到这里终于可以切读流量了。虽然我们之前对比过DB的数据，但是因为cache的关系，我们还需要调用上游服务，比较old_service和new_service服务的输出是否一致。</p>

    <p>这一步由上游的vod完成，服务本身并不需要做什么改动。</p>

    <p>对比过程也出现了很多不一致的情况，除了DB里确实有不一样的情况之外，主要是Redis里面的脏数据导致，处理也很简单，调用服务清理Redis的接口即可。</p>
  </li>
  <li>
    <p>上游所有读流量收敛到vod</p>

    <p>上游对比完成之后，我们就着手切换读流量了。</p>

    <p>首先确保所有读流量都收敛到vod服务——这主要是一些沟通方面的工作。</p>

    <p>事实上这一步放在切完流量之后也行，但是考虑到我们老服务不久之后就会下线，所以还是尽快让上游迁移比较好。</p>
  </li>
  <li>
    <p>上游切换读流量</p>

    <p>终于看到胜利的曙光，这最后一步，看起来也是最胆战心惊的一步。</p>

    <p>然而是用了三天左右的时间缓慢切完的。中间也基本没出任何纰漏。</p>

    <p>原因是上游的vod也作了重构，重构的版本正好读的是new_service服务。虽然我们的idl格式变了，但是上游vod把变化都包含在内了，对外暴露的接口没变。所以此处只需要切换vod两个版本之间的流量比就可以了。至于为什么可以直接切量，因为主要工作都在第8步对比过程完成了。</p>

    <p>那么为什么用了三天呢？因为需要充cache。如果直接全切，DB瞬间就被打挂了。</p>
  </li>
  <li>
    <p>代码重构</p>

    <p>切换完成之后，做了一次代码重构，虽然接口没变，但是因为存cache的数据结构都变了，上线也费了很多精力。同样省略。</p>

    <p>终于，我们的服务变成了想象中的样子。</p>
    <h1 id="写在最后">写在最后</h1>
    <p>没想到迁移一个DB竟然花了这么长时间。分享一下自己学到的东西。</p>

    <p>• 花费了太多精力来处理上线过程中各机器代码不一致的问题，如果我们能把微服务做到松耦合高内聚，流程尽量缩短，甚至只有一步（或者说各步骤之间没有依赖），那么无论多少台机器代码不一样，都不会有问题。</p>

    <p>• 服务收敛，不管是维护，还是切流量，都会简单很多。</p>

    <p>• 提前做好方案review，尽量简化流程，the simpler the better. 可以避免大多数问题。</p>

    <p>• 始终为自己的代码负责，尽量保持接口不变。这次迁移若不是vod兜底，又要花费许多时间来改写接入层。</p>

    <p>• 学习优秀代码设计，我们始终在路上。</p>
  </li>
</ol>

                        <ul class="pager">
                            
                            <li class="previous"><a href="/2017/09/07/readingCode.html"><span aria-hidden="true">&larr;</span>上一篇：我阅读代码的方法</a></li>
                            
                            
                            <li class="next"><a href="/2017/09/08/what-is-nsq.html">下一篇：NSQ是什么<span aria-hidden="true">&rarr;</span></a></li>
                            
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
