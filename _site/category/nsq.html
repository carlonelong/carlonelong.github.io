<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>来吧！</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css">
    <link href="/css/docs.css" rel="stylesheet" type="text/css">
    <link href="/css/syntax.css" rel="stylesheet" type="text/css">
    <link href="/css/theme.css" rel="stylesheet" type="text/css">
    <script src="/js/jquery.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/docs.min.js"></script>
</head>

<body>
<header id="top">
    <div class="row-fluid">
        <div class="navbar navbar-inverse" role="navigation">
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav">
                    <li class="active"><a href="/">Home</a></li>
                    <li class="active"><a href="/article">笔记</a></li>
                    <li class="active"><a href="/about">About</a></li>
                </ul>
            </div>
        </div>
    </div>
</header>


<div class="container-fluid">
    <div class="row">
        <div class="col-md-2 hidden-xs">
            <div class="sidebar well">
    
        <h1>笔记分类</h1>

<ul>
    <li><a href="/category/代码阅读.html">代码阅读（1）</a></li>
</ul>

<ul>
    <li><a href="/category/工作记录.html">工作记录（1）</a></li>
</ul>

<ul>
    <li><a href="/category/nsq.html">nsq（4）</a></li>
</ul>


    
</div>

        </div>
        <div class="col-md-8">
            
                
    

    

    
        
<div class="article">
    <div class="well">
        <h1 class="none"><a href="/2018/05/28/disk-queue.html">DiskQueue （2018年05月28日）</a></h1>
        <div class="content">在NSQ Topic一文中提到了，当消息堆积太多时，NSQ会把部分消息写入一个backend。这个backend实际是一个叫做diskQueue的数据结构。本文主要分析该结构。

同样，我们带着以下几个疑问来阅读代码。

  NSQ会在什么情况下把数据写入/读出diskQueue?
  diskQueue的数据是怎么存储的，如何保证数据正确读写？


diskQueue结构
照惯例先看一下diskQueue的数据结构
type diskQueue struct {
	// 64bit atomic vars need to be first for proper alignment on 32bit platforms

	// run-time state (also persisted to disk)
	readPos      int64
	writePos     int64
	readFileNum  int64
	writeFileNum int64
	depth        int64

	sync.RWMutex

	// instantiation time metadata
	name            string
	dataPath        string
	maxBytesPerFile int64 // currently this cannot change once created
	minMsgSize      int32
	maxMsgSize      int32
	syncEvery       int64         // number of writes per fsync
	syncTimeout     time.Duration // duration of time per fsync
	exitFlag        int32
	needSync        bool

	// keeps track of the position where we have read
	// (but not yet sent over readChan)
	nextReadPos     int64
	nextReadFileNum int64

	readFile  *os.File
	writeFile *os.File
	reader    *bufio.Reader
	writeBuf  bytes.Buffer

	// exposed via ReadChan()
	readChan chan []byte

	// internal channels
	writeChan         chan []byte
	writeResponseChan chan error
	emptyChan         chan int
	emptyResponseChan chan error
	exitChan          chan int
	exitSyncChan      chan int

	logf AppLogFunc
}


可见其内部结构很简单，包括一些读写文件的指针，当前读（写）到的文件位置，每个文件允许的最大大小等。根据这些信息我们可以推测：

  数据在diskQueue中是以磁盘文件形式存储的，并且这些文件的内容是连续的，可以通过不同的fileNum索引到。
  这些文件的大小相近。


diskQueue的创建
首先来看看diskQueue的创建流程，不出意外的话，会有一些load操作，比如读取当前正在读出（写入）数据的文件号，读（写）位置等。
func New(name string, dataPath string, maxBytesPerFile int64,
	minMsgSize int32, maxMsgSize int32,
	syncEvery int64, syncTimeout time.Duration, logf AppLogFunc) Interface {
	d := diskQueue{
		name:              name,
		...
	}

	// no need to lock here, nothing else could possibly be touching this instance
	err := d.retrieveMetaData()
	if err != nil &amp;&amp; !os.IsNotExist(err) {
		d.logf(ERROR, "DISKQUEUE(%s) failed to retrieveMetaData - %s", d.name, err)
	}

	go d.ioLoop()
	return &amp;d
}


不出所料，果然会调用retrieveMetaData来load数据，我们看看“meta data”都包含什么。
func (d *diskQueue) retrieveMetaData() error {
	var f *os.File
	var err error

	fileName := d.metaDataFileName()
	f, err = os.OpenFile(fileName, os.O_RDONLY, 0600)
	if err != nil {
		return err
	}
	defer f.Close()

	var depth int64
	_, err = fmt.Fscanf(f, "%d\n%d,%d\n%d,%d\n",
		&amp;depth,
		&amp;d.readFileNum, &amp;d.readPos,
		&amp;d.writeFileNum, &amp;d.writePos)
	if err != nil {
		return err
	}
	atomic.StoreInt64(&amp;d.depth, depth)
	d.nextReadFileNum = d.readFileNum
	d.nextReadPos = d.readPos

	return nil
}


答案很明显，包含当前diskQueue的消息数（depth），读（写）文件序号、读（写）位置。

自然有load的地方，肯定有对应save的地方，即persistMetaData，其调用在sync中。至于sync干了什么，稍后分析。

回到创建diskQueue的地方，除了retrieveMetaData之外，New还启动了一个ioLoop的goroutine。
ioLoop
func (d *diskQueue) ioLoop() {
	var dataRead []byte
	var err error
	var count int64
	var r chan []byte

	syncTicker := time.NewTicker(d.syncTimeout)

	for {
		// dont sync all the time :)
		if count == d.syncEvery {
			d.needSync = true
		}

		if d.needSync {
			err = d.sync()
			if err != nil {
				d.logf(ERROR, "DISKQUEUE(%s) failed to sync - %s", d.name, err)
			}
			count = 0
		}

		if (d.readFileNum &lt; d.writeFileNum) || (d.readPos &lt; d.writePos) {
			if d.nextReadPos == d.readPos {
				dataRead, err = d.readOne()
				if err != nil {
					d.logf(ERROR, "DISKQUEUE(%s) reading at %d of %s - %s",
						d.name, d.readPos, d.fileName(d.readFileNum), err)
					d.handleReadError()
					continue
				}
			}
			r = d.readChan
		} else {
			r = nil
		}

		select {
		// the Go channel spec dictates that nil channel operations (read or write)
		// in a select are skipped, we set r to d.readChan only when there is data to read
		case r &lt;- dataRead:
			count++
			// moveForward sets needSync flag if a file is removed
			d.moveForward()
		case &lt;-d.emptyChan:
			d.emptyResponseChan &lt;- d.deleteAllFiles()
			count = 0
		case dataWrite := &lt;-d.writeChan:
			count++
			d.writeResponseChan &lt;- d.writeOne(dataWrite)
		case &lt;-syncTicker.C:
			if count == 0 {
				// avoid sync when there's no activity
				continue
			}
			d.needSync = true
		case &lt;-d.exitChan:
			goto exit
		}
	}

exit:
	d.logf(INFO, "DISKQUEUE(%s): closing ... ioLoop", d.name)
	syncTicker.Stop()
	d.exitSyncChan &lt;- 1
}


ioLoop的逻辑稍微复杂一些，包括几部分

  如果需要同步，调用sync。
  如果有内容可读，则读取数据。
  监听若干chan，根据不同消息做出不同响应。


同步
func (d *diskQueue) sync() error {
	if d.writeFile != nil {
		err := d.writeFile.Sync()
		if err != nil {
			d.writeFile.Close()
			d.writeFile = nil
			return err
		}
	}

	err := d.persistMetaData()
	if err != nil {
		return err
	}

	d.needSync = false
	return nil
}


这里可以看到sync函数做的事情很简单：

  如果writeFile存在，刷新其内容到磁盘。
  保存元数据。


我们比较关心的是sync合适会被调用，跟踪needSync变量可知，有3种情况会触发同步。

  定时器触发，diskQueue会定期触发sync操作以保证数据正确性。
  读写一定数目（syncEvery）之后触发。
  一些特殊的读写事件，包括读取文件变化，读写失败等。


读数据
if (d.readFileNum &lt; d.writeFileNum) || (d.readPos &lt; d.writePos) {
	...
	r = d.readChan
} else {
	r = nil
}


这里有个很有意思的设置，如果当前可读，则将r设置为readChan，否则设置为nil。

在select中，空的chan会被跳过

所以如果读到数据，会写入readChan；否则会直接跳过。
来看看真正的读数据操作。
func (d *diskQueue) readOne() ([]byte, error) {
	var err error
	var msgSize int32

	if d.readFile == nil {
		curFileName := d.fileName(d.readFileNum)
		d.readFile, err = os.OpenFile(curFileName, os.O_RDONLY, 0600)
		if err != nil {
			return nil, err
		}

		d.logf(INFO, "DISKQUEUE(%s): readOne() opened %s", d.name, curFileName)

		if d.readPos &gt; 0 {
			_, err = d.readFile.Seek(d.readPos, 0)
			if err != nil {
				d.readFile.Close()
				d.readFile = nil
				return nil, err
			}
		}

		d.reader = bufio.NewReader(d.readFile)
	}

	err = binary.Read(d.reader, binary.BigEndian, &amp;msgSize)
	if err != nil {
		d.readFile.Close()
		d.readFile = nil
		return nil, err
	}

	if msgSize &lt; d.minMsgSize || msgSize &gt; d.maxMsgSize {
		// this file is corrupt and we have no reasonable guarantee on
		// where a new message should begin
		d.readFile.Close()
		d.readFile = nil
		return nil, fmt.Errorf("invalid message read size (%d)", msgSize)
	}

	readBuf := make([]byte, msgSize)
	_, err = io.ReadFull(d.reader, readBuf)
	if err != nil {
		d.readFile.Close()
		d.readFile = nil
		return nil, err
	}

	totalBytes := int64(4 + msgSize)

	// we only advance next* because we have not yet sent this to consumers
	// (where readFileNum, readPos will actually be advanced)
	d.nextReadPos = d.readPos + totalBytes
	d.nextReadFileNum = d.readFileNum

	// TODO: each data file should embed the maxBytesPerFile
	// as the first 8 bytes (at creation time) ensuring that
	// the value can change without affecting runtime
	if d.nextReadPos &gt; d.maxBytesPerFile {
		if d.readFile != nil {
			d.readFile.Close()
			d.readFile = nil
		}

		d.nextReadFileNum++
		d.nextReadPos = 0
	}

	return readBuf, nil
}


其实很简单，有以下几个步骤：

  如果当前没有读文件，则根据读序号打开文件。
  首先读出消息长度msgSize，然后根据msgSize。
  按需移动next/*指针，如果当前文件已读完，则移动到下一个文件。


这里可以看到其协议非常简单，就是把每个消息的长度写在改消息前面。因为都是本地读取，所以没有像IP/TCP那样设置各种各样的复杂字段。

readOne返回数据后，diskQueue会尝试把他们写入readChan中。如果写入成功，则更新真正的读写文件序号和位置，如下所示：
···go
func (d *diskQueue) moveForward() {
	oldReadFileNum := d.readFileNum
	d.readFileNum = d.nextReadFileNum
	d.readPos = d.nextReadPos
	depth := atomic.AddInt64(&amp;d.depth, -1)

// see if we need to clean up the old file
if oldReadFileNum != d.nextReadFileNum {
	// sync every time we start reading from a new file
	d.needSync = true

	fn := d.fileName(oldReadFileNum)
	err := os.Remove(fn)
	if err != nil {
		d.logf(ERROR, "DISKQUEUE(%s) failed to Remove(%s) - %s", d.name, fn, err)
	}
}

d.checkTailCorruption(depth) } ``` ### 写数据 上层可以调用Put来写入消息 ```go func (d *diskQueue) Put(data []byte) error {
d.RLock()
defer d.RUnlock()

if d.exitFlag == 1 {
	return errors.New("exiting")
}

d.writeChan &lt;- data
return &lt;-d.writeResponseChan } ``` 比较有意思的是用了两个chan来完成写操作，任意一个chan都会阻塞Put函数的返回。在ioLoop中当writeChan监听到数据写入时，会调用writeOne写入数据，然后将结果写回writeResponseChan。巧妙地用同步操作来实现了异步写入。



真正写入操作writeOne，因为其过程跟readOne非常类似，此处不再赘述。其中需要注意的地方是使用了writeBuf来合并数据长度和数据本身，一次性写入文件。避免只写入部分数据的问题。

错误处理
最后来看一看当读/写发生错误时diskQueue的处理。
首先，写入的错误处理非常简单，直接把错误抛给调用方即可。
而读的错误处理就要复杂许多，但其核心思想就是：跳过当前文件，读写下一个文件。无论是handleReadError还是checkTailCorruption都是这么做的。

diskQueue读写时机
前面介绍了diskQueue的各个操作的步骤，但是还没有涉及到上层调用。我们需要知道NSQ何时会读/写diskQueue的数据。
事实上从topic.go里面就能很轻易地看到答案。
func (t *Topic) put(m *Message) error {
	select {
	case t.memoryMsgChan &lt;- m:
	default:
		b := bufferPoolGet()
		err := writeMessageToBackend(b, m, t.backend)
		bufferPoolPut(b)
		t.ctx.nsqd.SetHealth(err)
		if err != nil {
			t.ctx.nsqd.logf(LOG_ERROR,
				"TOPIC(%s) ERROR: failed to write message to backend - %s",
				t.name, err)
			return err
		}
	}
	return nil
}


即在memoryMsgChan满了的时候将数据写入diskQueue。
func (t *Topic) messagePump() {
	...
	if len(chans) &gt; 0 {
		memoryMsgChan = t.memoryMsgChan
		backendChan = t.backend.ReadChan()
	}
	for {
		select {
		case msg = &lt;-memoryMsgChan:
		case buf = &lt;-backendChan:
			msg, err = decodeMessage(buf)
			if err != nil {
				t.ctx.nsqd.logf(LOG_ERROR, "failed to decode message - %s", err)
				continue
			}
	...
		}
	}
}


而读出则是在messagePump里面的无限for循环里，即只要有数据随时都可能被读出。

对于channel也类似，不再赘述。因为NSQ不保证消息有序，所以这种读写策略是完全可行的。

小结

  NSQ topic/channel会在内存数据channel memoryMsgChan满了之后将数据写入diskQueue。只要diskQueue不为空，就会随时从其中读出数据。
  diskQueue数据格式为消息长度+消息内容。当数据文件有一个地方出错，后面所有的消息都会丢失，这里可以设计更完善的策略尽量恢复更多消息。
  golang的chan是个很强大的feature，需要学习理解，融会贯通。

</div>
    </div>
</div>
        
<div class="article">
    <div class="well">
        <h1 class="none"><a href="/2017/10/01/nsq-channel.html">NSQ Channel （2017年10月01日）</a></h1>
        <div class="content">在NSQ Topic一文中提到了，一个topic下面可以注册多个channel，每个channel都有完整的数据拷贝，但是每个channel的单条信息只能被某一个用户接收。现在就来看看channel是如何实现的。
channel的定义
type Channel struct {
	// 64bit atomic vars need to be first for proper alignment on 32bit platforms
	requeueCount uint64
	messageCount uint64
	timeoutCount uint64

	sync.RWMutex

	topicName string
	name      string
	ctx       *context

	backend BackendQueue

	memoryMsgChan chan *Message
	exitFlag      int32
	exitMutex     sync.RWMutex

	// state tracking
	clients        map[int64]Consumer
	paused         int32
	ephemeral      bool
	deleteCallback func(*Channel)
	deleter        sync.Once

	// Stats tracking
	e2eProcessingLatencyStream *quantile.Quantile

	// TODO: these can be DRYd up
	deferredMessages map[MessageID]*pqueue.Item
	deferredPQ       pqueue.PriorityQueue
	deferredMutex    sync.Mutex
	inFlightMessages map[MessageID]*Message
	inFlightPQ       inFlightPqueue
	inFlightMutex    sync.Mutex
}


除了一些居家必备良药，如topicName，name等之外，channel跟topic一样，也有一个backend的磁盘队列。为什么topic和channel都需要队列呢？想象一下当topic下没有channel和topic下有多个channel的情况下消息都堆积在哪就豁然开朗了。

与topic相似，channel也有一些用来控制生命周期的变量，比如paused, ephemeral, deleteCallback。此外因为channel是直接连接到消费者，所以有一个clients字典来记录这些consumer。

然而这些跟消息分发半毛钱关系都没有，我们重点关注的是最后几个成员：inFlight*和deferred*。顾名思义，前者是正在分发过程中的消息的相关信息，后者则是被延迟发送消息的信息。

接下来来看看整个流程是怎么样的。
Channel消息分发
Consumer注册
func (c *Channel) AddClient(clientID int64, client Consumer) {
	c.Lock()
	defer c.Unlock()

	_, ok := c.clients[clientID]
	if ok {
		return
	}
	c.clients[clientID] = client
}


很简单，就是把新来的client加入到前面的clients中。不过有个问题，就是如果用户注册的channel不存在呢？回头看看topic就知道了，topic会为不存在的channel_name新创建一个channel。

func NewChannel(topicName string, channelName string, ctx *context,
	deleteCallback func(*Channel)) *Channel {

	c := &amp;Channel{
		topicName:      topicName,
		name:           channelName,
		memoryMsgChan:  make(chan *Message, ctx.nsqd.getOpts().MemQueueSize),
		clients:        make(map[int64]Consumer),
		deleteCallback: deleteCallback,
		ctx:            ctx,
	}
	if len(ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles) &gt; 0 {
		c.e2eProcessingLatencyStream = quantile.New(
			ctx.nsqd.getOpts().E2EProcessingLatencyWindowTime,
			ctx.nsqd.getOpts().E2EProcessingLatencyPercentiles,
		)
	}

	c.initPQ()

	if strings.HasSuffix(channelName, "#ephemeral") {
		c.ephemeral = true
		c.backend = newDummyBackendQueue()
	} else {
		dqLogf := func(level diskqueue.LogLevel, f string, args ...interface{}) {
			opts := ctx.nsqd.getOpts()
			lg.Logf(opts.Logger, opts.logLevel, lg.LogLevel(level), f, args...)
		}
		// backend names, for uniqueness, automatically include the topic...
		backendName := getBackendName(topicName, channelName)
		c.backend = diskqueue.New(
			backendName,
			ctx.nsqd.getOpts().DataPath,
			ctx.nsqd.getOpts().MaxBytesPerFile,
			int32(minValidMsgLength),
			int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength,
			ctx.nsqd.getOpts().SyncEvery,
			ctx.nsqd.getOpts().SyncTimeout,
			dqLogf,
		)
	}

	c.ctx.nsqd.Notify(c)

	return c
}


类似于topic，channel也会根据是否ephemeral来指定对应的backend。

值得注意的是，NSQ在初始化channel的时候也初始化了前面提到的inFlight和deferred变量。
func (c *Channel) initPQ() {
	pqSize := int(math.Max(1, float64(c.ctx.nsqd.getOpts().MemQueueSize)/10))

	c.inFlightMessages = make(map[MessageID]*Message)
	c.deferredMessages = make(map[MessageID]*pqueue.Item)

	c.inFlightMutex.Lock()
	c.inFlightPQ = newInFlightPqueue(pqSize)
	c.inFlightMutex.Unlock()

	c.deferredMutex.Lock()
	c.deferredPQ = pqueue.New(pqSize)
	c.deferredMutex.Unlock()
}


代码很清晰，但有两个问题：

  为什么需要两个变量Messages和PQ呢？
  为什么inFlightPQ和deferredPQ使用了不一样的数据结构，区别是什么？


我们接着往下面看。
分发消息到clients
还记得topic分发消息的方法（messagePump）吗？当消息到来的时候，会被拷贝分发到每个channel。如果消息不需要延迟发送，就调用PutMessage函数，否则调用PutMessageDeferred函数。

即时消息的转发
根据前面的分析我们大致能猜到，PutMessage操作inFlightMessages。跟踪一下PutMessage的代码：
func (c *Channel) PutMessage(m *Message) error {
	c.RLock()
	defer c.RUnlock()
	if c.Exiting() {
		return errors.New("exiting")
	}
	err := c.put(m)
	if err != nil {
		return err
	}
	atomic.AddUint64(&amp;c.messageCount, 1)
	return nil
}
func (c *Channel) put(m *Message) error {
	select {
	case c.memoryMsgChan &lt;- m:
	default:
		b := bufferPoolGet()
		err := writeMessageToBackend(b, m, c.backend)
		bufferPoolPut(b)
		c.ctx.nsqd.SetHealth(err)
		if err != nil {
			c.ctx.nsqd.logf(LOG_ERROR, "CHANNEL(%s): failed to write message to backend - %s",
				c.name, err)
			return err
		}
	}
	return nil
}


这部分跟topic分发消息几乎一样，但是消息写入memoryMsgChan之后就不知去向了。通过全局搜索可以发现在protocol_v2.go里面有跟topic又几乎一致的messagePump函数：
//代码很长，只列出处理消息的部分
func (p *protocolV2) messagePump(client *clientV2, startedChan chan bool) {
	//......
	for {
		select {
		case b := &lt;-backendMsgChan:
			if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate {
				continue
			}

			msg, err := decodeMessage(b)
			if err != nil {
				p.ctx.nsqd.logf(LOG_ERROR, "failed to decode message - %s", err)
				continue
			}
			msg.Attempts++

			subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout)
			client.SendingMessage()
			err = p.SendMessage(client, msg, &amp;buf)
			if err != nil {
				goto exit
			}
			flushed = false
		case msg := &lt;-memoryMsgChan:
			if sampleRate &gt; 0 &amp;&amp; rand.Int31n(100) &gt; sampleRate {
				continue
			}
			msg.Attempts++

			subChannel.StartInFlightTimeout(msg, client.ID, msgTimeout)
			client.SendingMessage()
			err = p.SendMessage(client, msg, &amp;buf)
			if err != nil {
				goto exit
			}
			flushed = false
		case &lt;-client.ExitChan:
			goto exit
		}
	}
}


可见，到这一步，无论是memory message还是backend message，都已经发送到client了，但是似乎并未用到inFlightMessages和inFlightPQ。不要着急，看看StartInFlightTimeout：
func (c *Channel) StartInFlightTimeout(msg *Message, clientID int64, timeout time.Duration) error {
	now := time.Now()
	msg.clientID = clientID
	msg.deliveryTS = now
	msg.pri = now.Add(timeout).UnixNano()
	err := c.pushInFlightMessage(msg)
	if err != nil {
		return err
	}
	c.addToInFlightPQ(msg)
	return nil
}


这里就很清楚了，会把发送到客户端的信息存到inFlightMessages和inFlightPQ。为什么？还记得NSQ的GUARANTEE-“messages are delivered at least once”么？没错，这是为了解决发送失败的问题。当一条消息被发送给client之后，如果在指定时间内仍没有收到客户端的响应的话，NSQ会把本次发送当做失败处理，然后把消息重新放回队列让别的用户有机会读到。
func (c *Channel) processInFlightQueue(t int64) bool {
	c.exitMutex.RLock()
	defer c.exitMutex.RUnlock()

	if c.Exiting() {
		return false
	}

	dirty := false
	for {
		c.inFlightMutex.Lock()
		msg, _ := c.inFlightPQ.PeekAndShift(t)
		c.inFlightMutex.Unlock()

		if msg == nil {
			goto exit
		}
		dirty = true

		_, err := c.popInFlightMessage(msg.clientID, msg.ID)
		if err != nil {
			goto exit
		}
		atomic.AddUint64(&amp;c.timeoutCount, 1)
		c.RLock()
		client, ok := c.clients[msg.clientID]
		c.RUnlock()
		if ok {
			client.TimedOutMessage()
		}
		c.put(msg)
	}

exit:
	return dirty
}


除此之外，client的一些请求，如FINISH、REQ、TOUCH等指令都会导致inFlightMessages和inFlightPQ的变化（删除或重排）。
到这里，可以解释为什么用使用PQ和map两种数据结构来保存message了：前者用时间为序，保存时间和message的对应关系，每次取最快要过期的消息，保证遍历能停在第一条未过期的消息处，减少不必要操作，提高效率；后者保存messageID和message对应关系，保证client操作（总是传入messageID）能在O(1)时间内找到message本身。

同样这也是为什么消息不保证顺序的一个重要原因。

还有几个小细节值得思考的：

  timeout时间是client设置的，因为每个client处于不一样的环境，所以这个选项留给了client自己配置。如果发现超时时间到了，消息还没处理完，有两个解决办法，TOUCH和设置更长的timeout。
  同一channel下每个用户收到的message是各不一样的，分配的过程就是go里面多个consumer listen同一个chan的过程。每个client可以设置一个sampleRate来决定有多大可能性接收发来的消息。
  每条消息会记录一个Attempt值，表示发送的次数。client可以根据这个值做一些处理，比如发送10次强制标记为完成。 NSQ是不会主动删除消息的。


延时消息的转发
前面已经大致描述了即时消息的转发流程，延时消息这里就简略一点。直接上代码。
func (c *Channel) PutMessageDeferred(msg *Message, timeout time.Duration) {
	atomic.AddUint64(&amp;c.messageCount, 1)
	c.StartDeferredTimeout(msg, timeout)
}
func (c *Channel) StartDeferredTimeout(msg *Message, timeout time.Duration) error {
	absTs := time.Now().Add(timeout).UnixNano()
	item := &amp;pqueue.Item{Value: msg, Priority: absTs}
	err := c.pushDeferredMessage(item)
	if err != nil {
		return err
	}
	c.addToDeferredPQ(item)
	return nil
}
func (c *Channel) processDeferredQueue(t int64) bool {
	c.exitMutex.RLock()
	defer c.exitMutex.RUnlock()

	if c.Exiting() {
		return false
	}

	dirty := false
	for {
		c.deferredMutex.Lock()
		item, _ := c.deferredPQ.PeekAndShift(t)
		c.deferredMutex.Unlock()

		if item == nil {
			goto exit
		}
		dirty = true

		msg := item.Value.(*Message)
		_, err := c.popDeferredMessage(msg.ID)
		if err != nil {
			goto exit
		}
		c.put(msg)
	}

exit:
	return dirty
}


这里就相对简单了，只要把消息存入deferredPQ，时间到了之后取出来，放入inFlightPQ即可。
那么为何deferredPQ没有使用和inFlightPQ一样的数据结构呢？说实话不是很明白，我的看法是，因为NSQ主要消耗资源是内存，inFlight message比deferred message存的数据较多，所以要区分开，尽量减小内存占用。但是一般情况下deferred message不会太多吧？
消息清理
某些情况下，channel关闭，需要处理剩下的消息，除非删除channel，否则都会会写到磁盘。
func (c *Channel) flush() error {
	var msgBuf bytes.Buffer

	if len(c.memoryMsgChan) &gt; 0 || len(c.inFlightMessages) &gt; 0 || len(c.deferredMessages) &gt; 0 {
		c.ctx.nsqd.logf(LOG_INFO, "CHANNEL(%s): flushing %d memory %d in-flight %d deferred messages to backend",
			c.name, len(c.memoryMsgChan), len(c.inFlightMessages), len(c.deferredMessages))
	}

	for {
		select {
		case msg := &lt;-c.memoryMsgChan:
			err := writeMessageToBackend(&amp;msgBuf, msg, c.backend)
			if err != nil {
				c.ctx.nsqd.logf(LOG_ERROR, "failed to write message to backend - %s", err)
			}
		default:
			goto finish
		}
	}

finish:
	c.inFlightMutex.Lock()
	for _, msg := range c.inFlightMessages {
		err := writeMessageToBackend(&amp;msgBuf, msg, c.backend)
		if err != nil {
			c.ctx.nsqd.logf(LOG_ERROR, "failed to write message to backend - %s", err)
		}
	}
	c.inFlightMutex.Unlock()

	c.deferredMutex.Lock()
	for _, item := range c.deferredMessages {
		msg := item.Value.(*Message)
		err := writeMessageToBackend(&amp;msgBuf, msg, c.backend)
		if err != nil {
			c.ctx.nsqd.logf(LOG_ERROR, "failed to write message to backend - %s", err)
		}
	}
	c.deferredMutex.Unlock()

	return nil
}


代码比较简单，就不赘述了。
小结
本文详细描述了NSQ的channel，大致总结成以下几点

  channel的每一条消息至少发送一次，至多发送给一个consumer。
  channel的消息是无序的。
  client可以通过设置timeout、sampleRate、maxAttemptCount等参数来过滤接收到的消息。
  写文章好累。

</div>
    </div>
</div>
        
<div class="article">
    <div class="well">
        <h1 class="none"><a href="/2017/09/17/nsq-topic.html">NSQ Topic （2017年09月17日）</a></h1>
        <div class="content">第一篇文章提到了，本着从简单到复杂的原则，我们会从nsqd开始对NSQ进行分析。
nsqd组成
先从官网上偷一张示意图



从图中可以看出

  一个topic下对应多个channel
  一个channel下对应多个consumer
  每个channel都会获得所有消息
  单个channel下的消息只会被一个consumer获得
  如果一个channel的消息一直不被消费，会一直堆积


我们可以得出以下几点推论：

  如果两个消费者都想要获得全部消息，那么他们必须在独立的channel下。比如硬盘备份和打印metrics。
  即使consumer在channel创建之后才注册，在堆积不超过nsqd前提下，仍然能获取到所有消息。


同样也会有疑问，比如：

  每个channel的堆积上限是多少？超过上限之后处理逻辑是什么？
  多个consumer消费同一个channel采用什么样的分配策略，是否公平？
  从topic到多个channel，从channel到多个consumer，如何保证消息不会丢失？
  如何保证同一channel下一条消息不会发送到多个consumer?


带着这些问题，我们进入代码的世界。
topic
topic的代码很好找，在nsqd/topic.go里面。首先看看topic的定义：
type Topic struct {
	// 64bit atomic vars need to be first for proper alignment on 32bit platforms
	messageCount uint64

	sync.RWMutex

	name              string
	channelMap        map[string]*Channel
	backend           BackendQueue
	memoryMsgChan     chan *Message
	exitChan          chan int
	channelUpdateChan chan int
	waitGroup         util.WaitGroupWrapper
	exitFlag          int32
	idFactory         *guidFactory

	ephemeral      bool
	deleteCallback func(*Topic)
	deleter        sync.Once

	paused    int32
	pauseChan chan bool

	ctx *context
}


不妨先根据字面意思和之前的分析来猜测一下，Topic里面各个字段的含义。
messageCount毫无疑问是消息的条数；name是topic的名字；channelMap是用来存储不同channel的；sync.RWMutex是读写锁，猜测是控制多个channel的同步用的……

几个chan的含义都比较明显，比较奇怪的是backend这个字段，看起来略显突兀，似乎和队列的堆积有些关系。ephemeral用来标志topic是临时有效的，不知道会有怎么样的处理呢？

那么从topic的生命周期来寻找答案吧。
create topic
func NewTopic(topicName string, ctx *context, deleteCallback func(*Topic)) *Topic {
	t := &amp;Topic{
		name:              topicName,
		channelMap:        make(map[string]*Channel),
		memoryMsgChan:     make(chan *Message, ctx.nsqd.getOpts().MemQueueSize),
		exitChan:          make(chan int),
		channelUpdateChan: make(chan int),
		ctx:               ctx,
		pauseChan:         make(chan bool),
		deleteCallback:    deleteCallback,
		idFactory:         NewGUIDFactory(ctx.nsqd.getOpts().ID),
	}

	if strings.HasSuffix(topicName, "#ephemeral") {
		t.ephemeral = true
		t.backend = newDummyBackendQueue()
	} else {
		dqLogf := func(level diskqueue.LogLevel, f string, args ...interface{}) {
			opts := ctx.nsqd.getOpts()
			lg.Logf(opts.Logger, opts.logLevel, lg.LogLevel(level), f, args...)
		}
		t.backend = diskqueue.New(
			topicName,
			ctx.nsqd.getOpts().DataPath,
			ctx.nsqd.getOpts().MaxBytesPerFile,
			int32(minValidMsgLength),
			int32(ctx.nsqd.getOpts().MaxMsgSize)+minValidMsgLength,
			ctx.nsqd.getOpts().SyncEvery,
			ctx.nsqd.getOpts().SyncTimeout,
			dqLogf,
		)
	}

	t.waitGroup.Wrap(func() { t.messagePump() })

	t.ctx.nsqd.Notify(t)

	return t
}


好吧，搞了半天也没搞定代码显示行号的问题。。。。。。

关注第14行，这里有对ephemeral的判断，如果topicName有这个后缀，那么backend就是假的，否则就会起一个diskqueue来做backend。从diskqueue的名字猜测，这是一个存储在磁盘上的队列。因此我们之前的猜测大体是正确的：topic的消息太多时，会存在磁盘里面。对这个diskqueue的分析，我们以后再做。

再看一个有意思的用法
func (w *WaitGroupWrapper) Wrap(cb func()) {
	w.Add(1)
	go func() {
		cb()
		w.Done()
	}()
}


这是waitGroup的Wrap操作，把cb封装起来。注意这里没有Wait操作，所以并不会阻塞。那么阻塞在哪里呢？搜代码可以发现，在topic的exit操作中。

也就是说，这个waitGroup的作用是：在退出topic之前，一定要保证cb（这里是messagePump）执行完成。从名字上看，messagePump就是topic不断往各个channel发送消息的过程，这个我们留在后面分析。
create channel
func (t *Topic) GetChannel(channelName string) *Channel {
	t.Lock()
	channel, isNew := t.getOrCreateChannel(channelName)
	t.Unlock()

	if isNew {
		// update messagePump state
		select {
		case t.channelUpdateChan &lt;- 1:
		case &lt;-t.exitChan:
		}
	}

	return channel
}


代码很清楚：给定一个channel name，如果对应的channel已经存在，就返回，否则创建一个新的。对于topic也有一样的逻辑（在nsqd中）。所以如果没有对topic和name做别的限制，你想怎么用就怎么用。这里的channelUpdateChan是用来告诉messagePump：我又创建了一个channel，下次别忘了把消息发给它。
接收消息
topic创建好之后，producer就可以往这个topic下面发消息了。
func (t *Topic) PutMessage(m *Message) error {
	t.RLock()
	defer t.RUnlock()
	if atomic.LoadInt32(&amp;t.exitFlag) == 1 {
		return errors.New("exiting")
	}
	err := t.put(m)
	if err != nil {
		return err
	}
	atomic.AddUint64(&amp;t.messageCount, 1)
	return nil
}

func (t *Topic) put(m *Message) error {
	select {
	case t.memoryMsgChan &lt;- m:
	default:
		b := bufferPoolGet()
		err := writeMessageToBackend(b, m, t.backend)
		bufferPoolPut(b)
		t.ctx.nsqd.SetHealth(err)
		if err != nil {
			t.ctx.nsqd.logf(LOG_ERROR,
				"TOPIC(%s) ERROR: failed to write message to backend - %s",
				t.name, err)
			return err
		}
	}
	return nil
}


这里的逻辑也很简单，消息来到后，尝试往memoryMsgChan里面写。如果阻塞了，就往backend里面写。所以，不需要担心消息写不下，只要磁盘有空间就没问题。那么怎么知道堆积了多少消息呢？
func (t *Topic) Depth() int64 {
	return int64(len(t.memoryMsgChan)) + t.backend.Depth()
}


那么写往memoryMsgChan的消息又哪儿去了呢？很显然，要发送到每个channel去了。怎么发的呢？是的，messagePump！
消息转发
终于到消息转发了，激不激动？
func (t *Topic) messagePump() {
	var msg *Message
	var buf []byte
	var err error
	var chans []*Channel
	var memoryMsgChan chan *Message
	var backendChan chan []byte

	t.RLock()
	for _, c := range t.channelMap {
		chans = append(chans, c)
	}
	t.RUnlock()

	if len(chans) &gt; 0 {
		memoryMsgChan = t.memoryMsgChan
		backendChan = t.backend.ReadChan()
	}

	for {
		select {
		case msg = &lt;-memoryMsgChan:
		case buf = &lt;-backendChan:
			msg, err = decodeMessage(buf)
			if err != nil {
				t.ctx.nsqd.logf(LOG_ERROR, "failed to decode message - %s", err)
				continue
			}
		case &lt;-t.channelUpdateChan:
			chans = chans[:0]
			t.RLock()
			for _, c := range t.channelMap {
				chans = append(chans, c)
			}
			t.RUnlock()
			if len(chans) == 0 || t.IsPaused() {
				memoryMsgChan = nil
				backendChan = nil
			} else {
				memoryMsgChan = t.memoryMsgChan
				backendChan = t.backend.ReadChan()
			}
			continue
		case pause := &lt;-t.pauseChan:
			if pause || len(chans) == 0 {
				memoryMsgChan = nil
				backendChan = nil
			} else {
				memoryMsgChan = t.memoryMsgChan
				backendChan = t.backend.ReadChan()
			}
			continue
		case &lt;-t.exitChan:
			goto exit
		}

		for i, channel := range chans {
			chanMsg := msg
			// copy the message because each channel
			// needs a unique instance but...
			// fastpath to avoid copy if its the first channel
			// (the topic already created the first copy)
			if i &gt; 0 {
				chanMsg = NewMessage(msg.ID, msg.Body)
				chanMsg.Timestamp = msg.Timestamp
				chanMsg.deferred = msg.deferred
			}
			if chanMsg.deferred != 0 {
				channel.PutMessageDeferred(chanMsg, chanMsg.deferred)
				continue
			}
			err := channel.PutMessage(chanMsg)
			if err != nil {
				t.ctx.nsqd.logf(LOG_ERROR,
					"TOPIC(%s) ERROR: failed to put msg(%s) to channel(%s) - %s",
					t.name, msg.ID, channel.name, err)
			}
		}
	}

exit:
	t.ctx.nsqd.logf(LOG_INFO, "TOPIC(%s): closing ... messagePump", t.name)
}


在一个比较大的for循环里，messagePump对几类消息做了不同的处理

  memoryMsgChan或者backendChan，表示真实的消息内容，区别在于后者是编码后存入磁盘的，所以需要多一道解码的工序。
  channelUpdateChan，前面提到过，这里是用来刷新channel的。
  pauseChan，pause和unpause都通过这个传进来，用于控制topic的消费。
  exitChan，退出（想想之前waitGroup)。


从memoryMsgChan或者backendChan拿到消息之后，自然是要写入channel的。根据消息是否deferred，有两种不同方式：

  如果不是deferred，直接写入channel
  否则写入channel的deferred priority queue


这里有个小细节，因为要发送到n个channel，原本已经有一个message了，所以把这个message发送到第0个channel，可以减少一次复制的开销。
从这里可以看到为什么每个channel都能获得所有消息了：他们得到的都是一份拷贝，并不是共享一份消息，这样各个channel之间就不会互相干扰了。
退出
topic有两种退出方式

  被delete，这种情况下清空并删除所有channel，通知nsqlookupd本topic已经被删除了。
  被正常close，这种情况下需要把所有数据都写入磁盘，然后关闭所有channel退出。


全景图
前面我们只是介绍了topic在整个生命周期中与别的部分进行的交互，但是还缺乏上层对topic的管理，下图描述了整个流程。


注意图中consumer注册到nsqd可以发生在nsqd启动后的任意时刻。

小结
本文跟踪了nsqd中topic的整个生命周期，分析了topic与channel的交互流程。
现在我们可以尝试着回答开始提出的问题的一小部分

  堆积上限就是磁盘的大小，超过上限的处理策略需要参看diskqueue的处理。
  单机情况下，每个channel都会收到每条消息的完整拷贝，所以不会丢失。多机的情况容以后分析。


接下来两篇文章将会分析diskqueue和channel。
</div>
    </div>
</div>
        
<div class="article">
    <div class="well">
        <h1 class="none"><a href="/2017/09/08/what-is-nsq.html">NSQ是什么 （2017年09月08日）</a></h1>
        <div class="content">这是NSQ代码阅读笔记的第一遍。之前在如何阅读代码一文中提到了阅读代码之前要先了解项目的用途和功能模块划分。本文试图记录这些信息。

我们为什么需要NSQ
我们在开发过程中多次用到了NSQ，所以我认为应该更加深入地了解它。
那么为什么需要用到NSQ呢？可能不同的人有不同的用法。我们使用了它的最主要的两个特性：解耦和缓冲。
比如工作流每执行一步之后发送到调用者的调用状态（成功or失败），转码后的消息回调，以及转码过程中更新完DB后更新Redis的操作，都是用NSQ来完成的。
至于为什么用NSQ而不是Kafka或者RabbitMQ之类，主要是因为我们这边都是go的代码，使用NSQ接入别的如监控等服务比较方便，同时让生态显得比较统一。

如果你使用一个消息队列，你最关心的特性是什么呢？
我们最关注的有这几点：

  消息即时性，反映在数据上就是pct99。
  消息完备性，即无论生产者发送了多少消息，消费者都能完全读到，不会丢失消息或者其中的某一部分。
  消息可重读，即多个消费者都能读出相同的数据（不特别过滤的条件下）
  容灾性能，即没有单点故障，能快速扩容，便于监控，支持降级，故障中能迅速恢复，不丢失数据。


查看NSQ的主页（http://nsq.io/overview/features_and_guarantees.html），我们能找到官方对于这些要求的描述。
Features
• support distributed topologies with no SPOF
• horizontally scalable (no brokers, seamlessly add more nodes to the cluster)
• low-latency push based message delivery (performance)
• combination load-balanced and multicast style message routing
• excel at both streaming (high-throughput) and job oriented (low-throughput) workloads
• primarily in-memory (beyond a high-water mark messages are transparently kept on disk)
• runtime discovery service for consumers to find producers (nsqlookupd)
• transport layer security (TLS)
• data format agnostic
• few dependencies (easy to deploy) and a sane, bounded, default configuration
• simple TCP protocol supporting client libraries in any language
• HTTP interface for stats, admin actions, and producers (no client library needed to publish)
• integrates with statsd for realtime instrumentation
• robust cluster administration interface (nsqadmin)

Guarantees
• messages are not durable (by default)
• messages are delivered at least once
• messages received are un-ordered
• consumers eventually find all topic producers


可见NSQ都能很好地满足我们的需求，同时还将稳定性放在了一个很重要的位置。在以后的若干篇文章内，我会根据代码来分析这些特性的实现。

你想象中的NSQ实现
一个典型的消息队列如何实现呢？如果你熟悉golang，一定会马上想到channel。它同样是一个生产者+消费者的结构，只要channel有数据，就能一直读取，只要channel未满，就能一直写入。其他情况都会阻塞。
事实上，NSQ最核心的数据结构确实是用channel来实现的。不过，需要增加许多额外的手段来保证上面提到的各种特性。
比如：为了实现多个消费者读到同样的数据，引入了单个topic下包含多个channel（此channel非go中的chan）的结构；为了保证消息不丢失，引入了diskQueue将数据保存在磁盘上；为了保证消息一定能被消费者完全接受，引入了inFlightQueue；为了实现延时消息，引入了deferedQueue。

阅读别人的代码最好从一个写代码的人的角度来思考：你要了解的这个功能是不是最基础的特性，如果不是，它依赖了哪些特性。就好像你在实现功能的时候先要找到别人提供的API一样。遇到看不懂的代码怎么办？先放在一边，了解了最基础的特性（函数，类之类）之后，再加上Google，一般来说就能很轻松地理解了。
NSQ的组成
NSQ由nsqd, nsqlookupd和nsqadmin3个守护进程（生产环境中是多个进程，但都是这3种之一）构成。
• nsqd is the daemon that receives, queues, and delivers messages to clients.
• nsqlookupd is the daemon that manages topology information and provides an eventually consistent discovery service.
• nsqadmin is a web UI to introspect the cluster in realtime (and perform various administrative tasks).


nsq是消息收取、存储和分发的主体，在没有其他两个进程的情况下也可以正常运行。

nsqlookupd用来管理nsqd进程的拓扑结构以及服务发现，比如nsqadmin就需要nsqlookupd来找到合适的nsqd进程。

nsqadmin就是用来操作和查看集群信息的UI界面。

事实上我们关注的核心功能都在nsqd中，所以接下来的分析我会先从nsqd开始，在必要的时候引入nsqdlookupd和nsqadmin。
小结
所以NSQ是什么呢？NSQ就是一个消息队列，它能保证消息迅速传达，能保证所有消息不会丢失(除非磁盘满了），至少被传递（到消费者）一次，它能保证多个消费者都能读到同样的数据，它没有单点故障，能快速从错误中恢复。还有一点，它的代码足够简单，而且很gopher范。
</div>
    </div>
</div>
        
    

            
            

            
        </div>
    </div>
</div>
